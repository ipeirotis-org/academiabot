{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis-org/academiabot/blob/master/05-Time_Series/A-Introduction_to_Time_Series_using_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJXQvkmz9pmk"
      },
      "source": [
        "# Introduction to Time Series and Forecasting\n",
        "\n",
        "*Based on the book [Introduction to Time Series and Forecasting](https://link.springer.com/book/10.1007/978-3-319-29854-2) by Brockwell and Davis.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "motivating_example"
      },
      "source": [
        "## Why Time Series Analysis Matters\n",
        "\n",
        "Your manager walks into your office and says:\n",
        "\n",
        "> *\"Sales are up 15% this month compared to last month. Great job!\"*\n",
        "\n",
        "But wait—is that actually good news? What if your product is ice cream, and you're comparing July to June? Or what if you're comparing December retail sales to November? Without understanding **seasonality**, you can't tell whether 15% growth is exceptional, expected, or actually disappointing.\n",
        "\n",
        "Time series analysis gives you the tools to answer questions like:\n",
        "- Is this month's performance actually above or below expectations?\n",
        "- What's the underlying trend, separate from seasonal fluctuations?\n",
        "- What should we expect next quarter? Next year?\n",
        "- Are there anomalies that require investigation?\n",
        "\n",
        "These skills are essential for anyone working with business data—whether you're forecasting demand, planning capacity, managing inventory, or evaluating performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. **Recognize and plot time-indexed data** using Pandas datetime functionality\n",
        "2. **Pull external time series data** via APIs and libraries (stocks, FRED economic data)\n",
        "3. **Normalize time series** by indexing and through inflation adjustment\n",
        "4. **Diagnose key patterns**: autocorrelation, seasonality, and trend\n",
        "5. **Apply window functions** (rolling, expanding, exponential) for trend extraction\n",
        "6. **Decompose time series** into trend, seasonal, and residual components\n",
        "7. **Change temporal granularity** using resampling"
      ],
      "metadata": {
        "id": "kzd1zNtwCOz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to change the project_id to your own Google Cloud project\n",
        "project_id = \"nyu-datasets\" # <<<<<< CHANGE THIS"
      ],
      "metadata": {
        "id": "wXAHhZo23XfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCYUqz2k9pmo"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "\n",
        "# This code snippet authenticates the user to access\n",
        "# Google Cloud services from within Colab. This is\n",
        "# necessary to interact with services like BigQuery.\n",
        "!pip install -q google-cloud-bigquery\n",
        "\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "!pip install -U -q yfinance fredapi\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from fredapi import Fred\n",
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting Setup\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# Change the graph defaults\n",
        "plt.rcParams['figure.figsize'] = (8, 3)  # Default figure size of 8x3 inches\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.color'] = 'lightgray'\n",
        "plt.rcParams['font.size'] = 10  # Default font size of 10 points\n",
        "plt.rcParams['lines.linewidth'] = 1  # Default line width of 1 points\n",
        "plt.rcParams['lines.markersize'] = 3  # Default marker size of 3 points\n",
        "plt.rcParams['legend.fontsize'] = 10  # Default legend font size of 10 points"
      ],
      "metadata": {
        "id": "0W0rt_1zaHEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 1: What is a Time Series?\n",
        "\n",
        "A time series is a set of observations $x_t$, each one being recorded at a specific time $t$. In our sessions, we focus on **discrete** time series, where observations are recorded at fixed time intervals (e.g., once an hour, or every 30 seconds, or every 7 days).\n",
        "\n",
        "We only consider **regular** time series, where the time between observations is constant (i.e., we do not consider account deposits or withdrawals from an ATM that happen at various times; these are examples of an irregular time series)."
      ],
      "metadata": {
        "id": "D79hThX7_yOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Core Components of a Time Series\n",
        "\n",
        "Most business time series can be broken down into three components:\n",
        "\n",
        "| Component | Description | Business Example |\n",
        "|-----------|-------------|------------------|\n",
        "| **Trend** | Long-term direction (up, down, flat) | Company revenue growing 8% annually |\n",
        "| **Seasonality** | Predictable, repeating patterns | Retail sales spike in Q4 every year |\n",
        "| **Residual** | Random noise after removing trend & seasonality | Unexpected supply chain disruption |\n",
        "\n",
        "Understanding these components helps us answer questions like:\n",
        "- \"Is our underlying business growing?\" (trend)\n",
        "- \"When should we staff up for peak demand?\" (seasonality)  \n",
        "- \"Was last Tuesday's drop an anomaly?\" (residual)"
      ],
      "metadata": {
        "id": "KI7CrL9Ub2ye"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrymQoHL9pmn"
      },
      "source": [
        "---\n",
        "## Part 2: Examples of Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB301JbH9pmq"
      },
      "source": [
        "### Example 1: Australian Red Wine Sales (Jan 1980 - Oct 1991)\n",
        "\n",
        "The file [`australian-wine-sales.txt`](https://storage.googleapis.com/datasets_nyu/australian-wine-sales.txt) contains the monthly sales of Australian red wines. Let's load and explore this classic time series dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_ybvQax9pmt"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/datasets_nyu/australian-wine-sales.txt\"\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# The `read_csv` command can read directly from a URL.\n",
        "# Since the file uses tab characters to separate columns,\n",
        "# we pass the `sep='\\t'` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDLr3O019pmt"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmOwjJ_b9pmu"
      },
      "outputs": [],
      "source": [
        "df.plot(\n",
        "    x = 'Date',\n",
        "    y = 'Sales'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the Australian Wine Sales Plot:**\n",
        "\n",
        "This plot visually represents monthly Australian red wine sales over time. We can observe two key features:\n",
        "\n",
        "1. **Upward Trend:** There's a general increase in sales over the years, suggesting growing demand or population growth. This long-term movement is the *trend* component. For a business, identifying this trend is crucial for long-term planning and capacity forecasting.\n",
        "\n",
        "2. **Seasonal Pattern:** Notice the recurring peaks and troughs within each year. Sales tend to be higher in certain months and lower in others. This predictable, repeating pattern is the *seasonal* component. Understanding this seasonality is vital for inventory management, staffing, and marketing campaigns. The peaks appear to be around July (winter in Australia), and the troughs around January (summer)."
      ],
      "metadata": {
        "id": "9ueouTsKWZ_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 2: NYC Vehicle Accidents\n",
        "\n",
        "Let's look at a different type of time series—one from BigQuery that you may recognize from earlier work."
      ],
      "metadata": {
        "id": "nj953FOng8uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of accidents per hour\n",
        "sql = '''\n",
        "  SELECT DATE_TRUNC(DATE_TIME, HOUR) AS acc_date, COUNT(*) AS accidents\n",
        "  FROM collisions.collisions\n",
        "  GROUP BY acc_date\n",
        "  ORDER BY acc_date\n",
        "'''\n",
        "\n",
        "# Read the results into Pandas\n",
        "acc_hourly = client.query(sql).to_dataframe()\n",
        "\n",
        "acc_hourly.plot(\n",
        "    kind='line',\n",
        "    x='acc_date',\n",
        "    y='accidents'\n",
        ")"
      ],
      "metadata": {
        "id": "o8J6P2kBZNLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the NYC Hourly Accidents Plot:**\n",
        "\n",
        "This plot shows vehicular accidents in NYC recorded on an hourly basis—a much more granular view than the wine sales.\n",
        "\n",
        "- **Volatility:** Unlike the relatively smooth wine sales trend, this plot is quite volatile, showing significant fluctuations from hour to hour.\n",
        "- **Potential Patterns:** While it's hard to see clear long-term trends at this resolution, we can observe periods of higher and lower activity. By aggregating this data, we can reveal more distinct patterns."
      ],
      "metadata": {
        "id": "Mh8J_EsUWfzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing Granularity with Resampling\n",
        "\n",
        "We can change the granularity of the time series using the `resample()` method. This is one of the most useful time series operations in Pandas.\n",
        "\n",
        "**Common Frequency Codes for Resampling:**\n",
        "\n",
        "| Code | Meaning | Example Use |\n",
        "|------|---------|-------------|\n",
        "| `'h'` or `'H'` | Hourly | High-frequency trading data |\n",
        "| `'D'` | Daily (calendar day) | Daily sales reports |\n",
        "| `'B'` | Business day | Workday-only analysis |\n",
        "| `'W'` | Weekly (Sunday end) | Weekly summaries |\n",
        "| `'MS'` | Month start | Monthly reporting (1st of month) |\n",
        "| `'ME'` | Month end | Monthly reporting (end of month) |\n",
        "| `'QE'` | Quarter end | Quarterly earnings |\n",
        "| `'YE'` | Year end | Annual reports |"
      ],
      "metadata": {
        "id": "SsqyzJbjifGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate to weekly data\n",
        "acc_weekly = acc_hourly.copy()\n",
        "\n",
        "# When we want to perform resampling, we need the datetime to be the \"index\"\n",
        "acc_weekly = acc_weekly.set_index('acc_date')\n",
        "\n",
        "# Resample to weekly frequency and sum the accidents\n",
        "acc_weekly = acc_weekly.resample('W').sum()\n",
        "\n",
        "# Plot the aggregated data; by default, x-axis is the index\n",
        "acc_weekly.plot(\n",
        "    kind='line',\n",
        "    y='accidents',\n",
        "    title='NYC Accidents (Weekly)'\n",
        ")"
      ],
      "metadata": {
        "id": "Yx4DqcGMiSrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the Aggregated Plot:**\n",
        "\n",
        "By aggregating hourly data to weekly, the plot becomes smoother and reveals clearer patterns:\n",
        "- **Smoothed Trend:** Weekly aggregation helps smooth out hourly noise\n",
        "- **Reduced Volatility:** Makes it easier to identify underlying movements\n",
        "- **Tradeoff:** Weekly seasonality (more accidents on certain days) is now hidden\n",
        "\n",
        "This highlights the importance of choosing the right aggregation level for the patterns you want to study."
      ],
      "metadata": {
        "id": "6gJ4mBwxW_1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3: US Population"
      ],
      "metadata": {
        "id": "tSxxbhu1pb_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the thousands=',' option to properly convert population numbers to integers\n",
        "population = pd.read_csv(\"https://storage.googleapis.com/datasets_nyu/us-population.txt\", sep=' ', thousands=',')\n",
        "population[\"Year\"] = pd.to_numeric(population[\"Year\"])\n",
        "population[\"US_Population\"] = pd.to_numeric(population[\"US_Population\"])\n",
        "\n",
        "population.plot(\n",
        "    kind = 'line',\n",
        "    x = 'Year',\n",
        "    y = 'US_Population',\n",
        "    marker = 'o'\n",
        ")"
      ],
      "metadata": {
        "id": "5tyB5wfzkU_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the US Population Plot:**\n",
        "\n",
        "- **Clear, Smooth Trend:** This is a classic example of a time series dominated by a strong, consistent upward trend. Population growth is generally a very smooth process.\n",
        "- **Absence of Seasonality:** As expected, annual population data does not show seasonality.\n",
        "- **Business Relevance:** Businesses operating in the US market would use this data for long-term market size forecasting and strategic planning."
      ],
      "metadata": {
        "id": "yHbvHyCEXJ6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 3: Working with Financial Data"
      ],
      "metadata": {
        "id": "HM6vYcs_pZuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stock Prices with Yahoo Finance\n",
        "\n",
        "The `yfinance` library provides easy access to stock price data. If you're interested in financial data analysis, [this article provides a comprehensive guide](https://towardsdatascience.com/a-comprehensive-guide-to-downloading-stock-prices-in-python-2cd93ff821d4)."
      ],
      "metadata": {
        "id": "stock_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df = yf.download(\n",
        "    tickers = ['GOOG','AAPL','MSFT', 'IBM'],\n",
        "    interval = '1d',        # download daily prices\n",
        "    start='2005-01-01',     # fetch prices after 2005\n",
        "    auto_adjust = True,     # adjust for splits etc\n",
        "    progress = False        # do not show a progress bar\n",
        ")['Close']  # Keep only the closing price\n",
        "\n",
        "display(stock_df)"
      ],
      "metadata": {
        "id": "Y17fnaQJo6Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.plot(title='Stock Prices (Raw)')"
      ],
      "metadata": {
        "id": "stock_raw_plot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.plot(title='Stock Prices (Raw, Log-scale)', logy=True)"
      ],
      "metadata": {
        "id": "lE4WOvwky63x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization: Indexing to a Base Date\n",
        "\n",
        "Raw stock prices are hard to compare because they started at different levels. A common technique is to **normalize** by dividing all values by the price on a base date. This shows relative performance.\n",
        "\n",
        "**Why normalize?** If Stock A goes from \\$10 to \\$20 and Stock B goes from \\$100 to \\$150, which performed better? Stock A doubled (100% return) while Stock B gained 50%. Normalization makes this clear."
      ],
      "metadata": {
        "id": "normalization_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first valid value for each stock\n",
        "first_values = stock_df.bfill().iloc[0]\n",
        "\n",
        "# Normalize: divide each column by its first value\n",
        "normalized_stock_df = stock_df / first_values\n",
        "\n",
        "normalized_stock_df.plot(title='Normalized Stock Prices (Base = First Trading Day)', logy=True)"
      ],
      "metadata": {
        "id": "normalized_plot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can clearly see that Apple and Google have significantly outperformed IBM and Microsoft over this period (though Microsoft has caught up recently)."
      ],
      "metadata": {
        "id": "normalized_interpretation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Returns\n",
        "\n",
        "For financial analysis, we often care about **returns** (percentage changes) rather than raw prices. The `pct_change()` method calculates the percentage change from the previous period."
      ],
      "metadata": {
        "id": "returns_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate daily returns\n",
        "daily_returns = stock_df.pct_change()\n",
        "\n",
        "# Show summary statistics\n",
        "daily_returns.describe()"
      ],
      "metadata": {
        "id": "daily_returns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of returns\n",
        "daily_returns.hist(bins=100, figsize=(10, 6), alpha=0.7)\n",
        "plt.suptitle('Distribution of Daily Returns')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "returns_hist"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 4: Economic Data from FRED\n",
        "\n",
        "The [FRED Economic Data by the Federal Reserve Bank of St Louis](https://fred.stlouisfed.org/) publishes a rich set of 822,000 US and international time series from 110 sources. This is an invaluable resource for business analysis."
      ],
      "metadata": {
        "id": "DycG4d3XiOPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fred = Fred(api_key='c041995ed8b9ab9c3f475e2ca8f7c288')"
      ],
      "metadata": {
        "id": "iPcAIhXMpOfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consumer Price Index for All Urban Consumers, not seasonally-adjusted\n",
        "# https://fred.stlouisfed.org/series/CPIAUCNS\n",
        "\n",
        "cpi = fred.get_series('CPIAUCNS')\n",
        "cpi.plot(title='Consumer Price Index (CPI)')"
      ],
      "metadata": {
        "id": "3p6s4XHfiQ7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the CPI Plot:**\n",
        "\n",
        "- **Upward Trend:** There is a clear and persistent upward trend, indicating that the general price level has increased significantly over time. This is inflation.\n",
        "- **Varying Slope:** The slope isn't constant—there are periods of higher inflation (steeper slope) and lower inflation (flatter slope).\n",
        "- **Business Relevance:** Tracking CPI is essential for pricing strategies, cost management, wage negotiations, and understanding purchasing power."
      ],
      "metadata": {
        "id": "q3Za9_1sZT97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Price increases compound exponentially,\n",
        "# so a log-scale helps us understand relative changes better\n",
        "cpi.plot(logy=True, title='CPI (Log Scale)')"
      ],
      "metadata": {
        "id": "-a3UKfeOjk7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why use a log scale?** On a log scale, a straight line indicates a constant *percentage* growth rate. Deviations from a straight line show periods where inflation was accelerating or decelerating."
      ],
      "metadata": {
        "id": "oJEFwsTAZPPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPI data is monthly. Calculate the 12-month percentage change\n",
        "# This is the standard way inflation is reported (year-over-year)\n",
        "cpi.pct_change(12).plot(title='Annual Inflation Rate (12-month % change in CPI)')"
      ],
      "metadata": {
        "id": "vTT5vCZzjePH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Activity 1: Normalizing with Inflation Adjustment\n",
        "\n",
        "**Scenario:** You want to analyze whether retail sales have truly grown in \"real\" terms (after accounting for inflation) or just in nominal terms.\n",
        "\n",
        "**Tasks:**\n",
        "1. Retrieve and plot the [Advance Retail Sales: Retail Trade and Food Services](https://fred.stlouisfed.org/series/RSAFSNA) series from FRED. The series code is `'RSAFSNA'`.\n",
        "2. Normalize the retail sales values to account for inflation using the CPI time series.\n",
        "\n",
        "**Hint:** To adjust for inflation, divide nominal values by the CPI index. You may need to align the time series (both are monthly, but ensure they match up)."
      ],
      "metadata": {
        "id": "DfUpL4-wqqpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n"
      ],
      "metadata": {
        "id": "IEJnifFqqiDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity 1 Solution"
      ],
      "metadata": {
        "id": "activity1_solution_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Activity 1 Solution (click to expand)\n",
        "\n",
        "# Step 1: Get retail sales data\n",
        "retail_sales = fred.get_series('RSAFSNA')\n",
        "\n",
        "# Plot nominal retail sales\n",
        "retail_sales.plot(title='Retail Sales (Nominal)', label='Nominal')\n",
        "plt.ylabel('Millions of Dollars')\n",
        "plt.show()\n",
        "\n",
        "# Step 2: Normalize for inflation\n",
        "# First, align the series by using a common date range\n",
        "# CPI is indexed with a base period (typically 1982-84 = 100)\n",
        "\n",
        "# Get the CPI value at the start of retail sales data to use as base\n",
        "common_start = max(retail_sales.index.min(), cpi.index.min())\n",
        "common_end = min(retail_sales.index.max(), cpi.index.max())\n",
        "\n",
        "# Filter both series to common range\n",
        "retail_aligned = retail_sales[common_start:common_end]\n",
        "cpi_aligned = cpi[common_start:common_end]\n",
        "\n",
        "# Normalize CPI to first period = 100 for easier interpretation\n",
        "cpi_normalized = cpi_aligned / cpi_aligned.iloc[0] * 100\n",
        "\n",
        "# Calculate real (inflation-adjusted) retail sales\n",
        "real_retail_sales = retail_aligned / cpi_normalized * 100\n",
        "\n",
        "# Plot both nominal and real\n",
        "plt.figure(figsize=(10, 4))\n",
        "retail_aligned.plot(label='Nominal Sales')\n",
        "real_retail_sales.plot(label='Real Sales (Inflation-Adjusted)')\n",
        "plt.title('Retail Sales: Nominal vs. Real')\n",
        "plt.ylabel('Index')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Insight: Real growth is smaller than nominal growth because\")\n",
        "print(\"part of the nominal increase is due to inflation, not actual volume growth.\")"
      ],
      "metadata": {
        "id": "activity1_solution"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 5: Analyzing Autocorrelation\n",
        "\n",
        "A commonly analyzed property of a time series is **autocorrelation**—the correlation of the series with lagged versions of itself. Simply stated, high autocorrelation means that if we know the value at time $t$, we can predict well the value at $t+1$."
      ],
      "metadata": {
        "id": "5nysr3TywnJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the wine sales data and prepare it for analysis\n",
        "url = \"https://storage.googleapis.com/datasets_nyu/australian-wine-sales.txt\"\n",
        "df = pd.read_csv(url, sep='\\t')\n",
        "\n",
        "# Convert the string date to proper datetime format\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df = df.set_index(\"Date\")\n",
        "df[\"Sales\"] = pd.to_numeric(df[\"Sales\"])"
      ],
      "metadata": {
        "id": "fAj9dgsiahlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autocorrelation at lag 1 (correlation between t and t+1)\n",
        "print(f\"Autocorrelation at lag 1: {df['Sales'].autocorr(lag=1):.3f}\")\n",
        "print(f\"Autocorrelation at lag 12: {df['Sales'].autocorr(lag=12):.3f}\")"
      ],
      "metadata": {
        "id": "TpIx6XNQxBbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting Autocorrelation:**\n",
        "- Lag 1 (~0.73): Sales this month are strongly correlated with sales last month\n",
        "- Lag 12 (~0.94): Sales this month are *very* strongly correlated with sales from the same month last year—this indicates strong seasonality!"
      ],
      "metadata": {
        "id": "o1S-QwxLxgUx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX8gIodj9pmw"
      },
      "source": [
        "### Lag Plots\n",
        "\n",
        "A **lag plot** visualizes the relationship between a time series and its lagged values. If there is autocorrelation, we'll see a pattern; if there's no autocorrelation (pure noise), we'll see a random scatter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_U1MAtJ9pmw"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Lag 1 plot\n",
        "pd.plotting.lag_plot(df[\"Sales\"], lag=1, c='b', ax=axes[0])\n",
        "axes[0].set_title(f'Lag 1 (autocorr = {df[\"Sales\"].autocorr(lag=1):.2f})')\n",
        "\n",
        "# Lag 12 plot\n",
        "pd.plotting.lag_plot(df[\"Sales\"], lag=12, c='r', ax=axes[1])\n",
        "axes[1].set_title(f'Lag 12 (autocorr = {df[\"Sales\"].autocorr(lag=12):.2f})')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that lag 12 shows a tighter correlation (less spread) than lag 1, confirming the strong annual seasonality in wine sales."
      ],
      "metadata": {
        "id": "v8hotHdoyq5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interactive Lag Plot\n",
        "And here is a more interactive plot that allows you to change the lag value using a slider"
      ],
      "metadata": {
        "id": "sJObf26g1osr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "lag = widgets.IntSlider(min=1,max=36,value=12)\n",
        "\n",
        "def update_plot(l):\n",
        "    fig, ax = plt.subplots(figsize=(5, 5)) # Create a new figure and axes\n",
        "    # Create a DataFrame for regplot with lagged data\n",
        "    lagged_data = pd.DataFrame({\n",
        "        'y(t)': df['Sales'],\n",
        "        f'y(t + {l})': df['Sales'].shift(-l)\n",
        "    }).dropna() # Drop rows with NaN values created by shifting\n",
        "\n",
        "    if not lagged_data.empty:\n",
        "        sns.regplot(x='y(t)', y=f'y(t + {l})', data=lagged_data, scatter_kws={'color': 'b'}, line_kws={'color': 'red'}, ax=ax)\n",
        "        ax.set_title(f'Lag Plot with Regression Line (Lag = {l})')\n",
        "        ax.set_xlabel('y(t)')\n",
        "        ax.set_ylabel(f'y(t + {l})')\n",
        "        ax.set_xlim(500, 3000) # Set x-axis limits\n",
        "        ax.set_ylim(500, 3000) # Set y-axis limits\n",
        "        ax.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No data available for lag = {l}\")\n",
        "        plt.close(fig) # Close the empty figure\n",
        "\n",
        "widgets.interact(update_plot, l=lag)"
      ],
      "metadata": {
        "id": "13lOCtN81mjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axNcEjbR9pmx"
      },
      "source": [
        "### Autocorrelation Plot (ACF)\n",
        "\n",
        "The **autocorrelation plot** shows the correlation value for many different lag values at once. This is a powerful diagnostic tool for identifying seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmk93mL9pmy"
      },
      "outputs": [],
      "source": [
        "pd.plotting.autocorrelation_plot(df[\"Sales\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the ACF Plot:**\n",
        "\n",
        "- **High Correlation at Small Lags:** Sales in one month are strongly correlated with recent months—this suggests a smooth, trending series.\n",
        "- **Peaks at Lag 12, 24, 36:** The prominent peaks at multiples of 12 confirm strong annual seasonality. Sales are highly correlated with the same month in previous years.\n",
        "- **Business Insight:** This tells us that past sales (especially 12 months ago) are very good predictors of current sales—crucial for seasonal forecasting and inventory management."
      ],
      "metadata": {
        "id": "5GIfjYKWoPqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 6: Seasonal Decomposition\n",
        "\n",
        "Now let's formally decompose the time series into its components: **Trend**, **Seasonality**, and **Residual**.\n",
        "\n",
        "There are two common models:\n",
        "- **Additive:** $Y_t = T_t + S_t + R_t$ (seasonal effect is constant in absolute terms)\n",
        "- **Multiplicative:** $Y_t = T_t \\times S_t \\times R_t$ (seasonal effect scales with the trend)\n",
        "\n",
        "For the wine sales data, since the seasonal swings get larger as the trend increases, a multiplicative model is more appropriate."
      ],
      "metadata": {
        "id": "gmir3VtH9pmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGClDRKA9pmz"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Decompose with 12-month periodicity (monthly data with annual seasonality)\n",
        "decomposition = seasonal_decompose(\n",
        "    x = df['Sales'],\n",
        "    model='multiplicative',\n",
        "    period=12,\n",
        "    extrapolate_trend=24\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10, 8))\n",
        "fig = decomposition.plot(\n",
        "    observed=False,\n",
        "    seasonal=True,\n",
        "    trend=True,\n",
        "    resid=True,\n",
        ")"
      ],
      "metadata": {
        "id": "yDcFdz7wtk1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the Decomposition:**\n",
        "\n",
        "- **Trend:** The steady upward movement in wine sales over the period. This is what we'd use for long-term planning.\n",
        "- **Seasonal:** The repeating annual pattern—higher in winter months (remember, Australia!), lower in summer. This is essential for short-term planning like inventory and staffing.\n",
        "- **Residual:** What's left after removing trend and seasonality. Ideally, this should look like random noise. Large spikes might indicate anomalies worth investigating."
      ],
      "metadata": {
        "id": "3QTNKkWZahqN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr8pLOvd9pm0"
      },
      "outputs": [],
      "source": [
        "# Access individual components\n",
        "decomposition.trend.plot(title='Trend Component')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The seasonal component repeats every 12 months\n",
        "decomposition.seasonal.head(24).plot(title='Seasonal Component (2 years)')"
      ],
      "metadata": {
        "id": "seasonal_plot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 7: Window Functions for Trend Analysis\n",
        "\n",
        "Another approach to extracting trends is using **window functions** that operate over a set of continuous time series points. These are especially useful when you want more control over how the trend is calculated.\n",
        "\n",
        "| Function | Description | Use Case |\n",
        "|----------|-------------|----------|\n",
        "| `rolling(n)` | Fixed window of n periods | Smooth out noise, extract trend |\n",
        "| `expanding()` | Window grows from start | Cumulative statistics |\n",
        "| `ewm(halflife=n)` | Exponentially weighted | More weight on recent data |"
      ],
      "metadata": {
        "id": "ly8PGn_k9pm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqelKNEv9pm7"
      },
      "outputs": [],
      "source": [
        "# Compare different window functions\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Raw data (semi-transparent)\n",
        "df['Sales'].plot(label='Raw', linestyle=\"--\", alpha=0.25)\n",
        "\n",
        "# 12-month moving average (removes seasonality)\n",
        "df['Sales'].rolling(12).mean().plot(label='12M Rolling Mean', alpha=0.75)\n",
        "\n",
        "# Expanding mean (cumulative average from start)\n",
        "df['Sales'].expanding().mean().plot(label='Expanding Mean', alpha=0.75)\n",
        "\n",
        "# Exponentially weighted moving average\n",
        "df['Sales'].ewm(halflife=12).mean().plot(label='EWMA (halflife 12M)', alpha=0.75)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1, 0.5))\n",
        "plt.title('Comparing Window Functions')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Insight:** The 12-month rolling mean removes the annual seasonality entirely (since it averages over a complete cycle), leaving just the underlying trend. This is why the rolling window size should match the seasonal period."
      ],
      "metadata": {
        "id": "window_insight"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Activity 2: Analyze NYC Accidents\n",
        "\n",
        "**Scenario:** The NYC Department of Transportation wants to understand accident patterns for resource allocation.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the daily NYC accident data (provided below)\n",
        "2. Create an autocorrelation plot and identify the key periodicities\n",
        "3. Decompose the time series using `seasonal_decompose` with an appropriate period\n",
        "4. What days of the week have the highest/lowest accident rates?\n",
        "\n",
        "**Hint:** For daily data with weekly seasonality, use `period=7`"
      ],
      "metadata": {
        "id": "Bo8bJ5RN9pm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load daily accident data\n",
        "sql = '''\n",
        "  SELECT DATE_TRUNC(DATE_TIME, DAY) AS acc_date, COUNT(*) AS accidents\n",
        "  FROM collisions.collisions\n",
        "  GROUP BY acc_date\n",
        "  ORDER BY acc_date\n",
        "'''\n",
        "\n",
        "acc = client.query(sql).to_dataframe()\n",
        "acc['acc_date'] = pd.to_datetime(acc['acc_date'])\n",
        "acc = acc.set_index('acc_date')\n",
        "\n",
        "acc.plot(title='NYC Daily Accidents')"
      ],
      "metadata": {
        "id": "rpVw1x6dEv0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your analysis here\n"
      ],
      "metadata": {
        "id": "activity2_work"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity 2 Solution"
      ],
      "metadata": {
        "id": "activity2_solution_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Activity 2 Solution (click to expand)\n",
        "\n",
        "# Step 1: Autocorrelation analysis\n",
        "print(\"Key autocorrelations:\")\n",
        "print(f\"  Lag 1 (day-to-day): {acc['accidents'].autocorr(lag=1):.3f}\")\n",
        "print(f\"  Lag 7 (week-to-week): {acc['accidents'].autocorr(lag=7):.3f}\")\n",
        "print(f\"  Lag 365 (year-to-year): {acc['accidents'].autocorr(lag=365):.3f}\")\n",
        "\n",
        "# ACF plot\n",
        "plot = pd.plotting.autocorrelation_plot(acc['accidents'])\n",
        "plot.set_xlim(0, 90)\n",
        "plt.title('Autocorrelation (first 90 days)')\n",
        "plt.show()\n",
        "\n",
        "# Step 2: Seasonal decomposition with period=7 (weekly)\n",
        "decomposition = seasonal_decompose(\n",
        "    x=acc['accidents'],\n",
        "    model='multiplicative',\n",
        "    period=7,\n",
        "    extrapolate_trend=7\n",
        ")\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "fig = decomposition.plot(\n",
        "    observed=False,\n",
        "    seasonal=True,\n",
        "    trend=True,\n",
        "    resid=True,\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Weekly pattern analysis\n",
        "print(\"\\nWeekly Seasonal Pattern:\")\n",
        "weekly_pattern = decomposition.seasonal.head(7)\n",
        "weekly_pattern.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "print(weekly_pattern)\n",
        "\n",
        "weekly_pattern.plot(kind='bar', title='Weekly Seasonality Pattern')\n",
        "plt.ylabel('Seasonal Factor (1.0 = average)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Finding: Accidents are highest on Fridays and lowest on Sundays.\")\n",
        "print(\"This suggests more traffic enforcement resources should be deployed on Fridays.\")"
      ],
      "metadata": {
        "id": "activity2_solution"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Activity 3: ETF Sector Analysis\n",
        "\n",
        "**Scenario:** You're a portfolio analyst comparing the performance of different market sectors.\n",
        "\n",
        "Here are sector ETFs representing different areas of the economy (GICS classification):\n",
        "\n",
        "| Ticker | Sector | Description |\n",
        "|--------|--------|-------------|\n",
        "| XLK | Technology | Tech companies |\n",
        "| XLV | Healthcare | Pharma, medical devices |\n",
        "| XLF | Financials | Banks, insurance |\n",
        "| XLE | Energy | Oil, gas, renewables |\n",
        "| XLRE | Real Estate | REITs |\n",
        "\n",
        "**Tasks:**\n",
        "1. Download these 5 ETFs since January 1, 2010\n",
        "2. Normalize their prices to compare relative performance\n",
        "3. Calculate daily returns and examine their correlations\n",
        "4. Which sectors move together? Which provide diversification?\n",
        "5. Visualize the return correlation using pairwise scatterplots (use the a `seaborn.pairplot(... kind='reg')` command).\n",
        "\n"
      ],
      "metadata": {
        "id": "oKTGbBYRaa0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "etf_tickers = ['XLK', 'XLV', 'XLF', 'XLE', 'XLRE']\n",
        "etf_names = ['Tech', 'Healthcare', 'Financials', 'Energy', 'Real Estate']\n",
        "\n",
        "# Your code here\n",
        "\n",
        "# Here is a command that makes the regression line red, and makes\n",
        "# the scatterplots more transparent.\n",
        "\n",
        "# sns.pairplot(returns, kind='reg', plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.01}}`\n"
      ],
      "metadata": {
        "id": "T4AuoaoFhCta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activity 3 Solution"
      ],
      "metadata": {
        "id": "activity3_solution_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Activity 3 Solution (click to expand)\n",
        "\n",
        "# Step 1: Download ETF data\n",
        "etf_tickers = ['XLK', 'XLV', 'XLF', 'XLE', 'XLRE']\n",
        "etf_names = {\n",
        "    'XLK':'Tech',\n",
        "    'XLV':'Healthcare',\n",
        "    'XLF':'Financials',\n",
        "    'XLE':'Energy',\n",
        "    'XLRE':'Real Estate'\n",
        "}\n",
        "\n",
        "etf_df = yf.download(\n",
        "    tickers=etf_tickers,\n",
        "    start='2010-01-01',\n",
        "    auto_adjust=True,\n",
        "    progress=False\n",
        ")['Close']\n",
        "\n",
        "# Put user-friendly column names\n",
        "etf_df = etf_df.rename(columns=etf_names)\n",
        "\n",
        "# Step 2: Normalize prices\n",
        "first_values = etf_df.bfill().iloc[0] # backfill NaN values and pick first row\n",
        "normalized_etf = etf_df / first_values\n",
        "\n",
        "normalized_etf.plot(figsize=(10, 4), title='Normalized ETF Performance (Base = Jan 2010)')\n",
        "plt.ylabel('Growth Multiple')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Calculate returns and correlations\n",
        "returns_etf = etf_df.pct_change().dropna()\n",
        "returns_etf.columns = etf_names\n",
        "\n",
        "print(\"Return Statistics:\")\n",
        "display(returns_etf.describe().round(4))\n",
        "\n",
        "# Step 4: Correlation matrix\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "corr_matrix = returns_etf.corr()\n",
        "display(corr_matrix.round(3))\n",
        "\n",
        "# Visualize with heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, fmt='.2f')\n",
        "plt.title('Sector Return Correlations')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"- Energy has the lowest correlation with other sectors → good for diversification\")\n",
        "print(\"- Tech, Healthcare, and Financials are moderately correlated\")\n",
        "print(\"- Real Estate correlates most with Financials (both interest-rate sensitive)\")"
      ],
      "metadata": {
        "id": "activity3_solution"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 8: From Analysis to Forecasting (Preview)\n",
        "\n",
        "Everything we've learned so far—identifying trends, seasonality, and autocorrelation—sets the foundation for **forecasting**. While detailed forecasting is beyond this notebook, here's a preview of how these concepts connect.\n",
        "\n",
        "### Simple Forecasting Approaches\n",
        "\n",
        "| Method | Idea | When to Use |\n",
        "|--------|------|-------------|\n",
        "| **Naive** | Next value = last value | Random walk data (like stock prices) |\n",
        "| **Seasonal Naive** | Next value = same period last year | Strong seasonality, stable trend |\n",
        "| **Trend + Seasonal** | Extrapolate trend, add seasonal pattern | Clear trend and seasonality |"
      ],
      "metadata": {
        "id": "forecasting_preview"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple seasonal naive forecast for wine sales\n",
        "# Forecast: next month's sales = same month last year's sales\n",
        "\n",
        "decomposition = seasonal_decompose(\n",
        "    x = df['Sales'],\n",
        "    model='multiplicative',\n",
        "    period=12,\n",
        "    extrapolate_trend=24\n",
        ")\n",
        "\n",
        "# Get the last 12 months of actual data\n",
        "last_year = df['Sales'].tail(12)\n",
        "\n",
        "# Use decomposition to make a simple forecast\n",
        "# Forecast = Latest Trend × Seasonal Factor\n",
        "\n",
        "latest_trend = decomposition.trend.dropna().iloc[-1]\n",
        "seasonal_factors = decomposition.seasonal.tail(12)\n",
        "\n",
        "# Create forecast for next 12 months\n",
        "forecast = latest_trend * seasonal_factors.values\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "df['Sales'].tail(36).plot(label='Actual', marker='o')\n",
        "pd.Series(\n",
        "    forecast,\n",
        "    index=pd.date_range(start=df.index[-1] + pd.DateOffset(months=1), periods=12, freq='MS')\n",
        ").plot(label='Simple Forecast', linestyle='--', marker='s')\n",
        "plt.title('Simple Seasonal Forecast')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"This simple forecast assumes the trend stays flat.\")\n",
        "print(\"More sophisticated methods (ARIMA, Prophet, etc.) can model trend changes.\")"
      ],
      "metadata": {
        "id": "simple_forecast"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps for Forecasting\n",
        "\n",
        "To build production-quality forecasts, you would typically:\n",
        "\n",
        "1. **Split data** into training and test sets (hold out recent data)\n",
        "2. **Choose a model** based on the patterns you've identified:\n",
        "   - ARIMA for autocorrelated data\n",
        "   - Prophet for data with multiple seasonalities\n",
        "   - Exponential smoothing for trending data\n",
        "3. **Evaluate forecast accuracy** using metrics like MAPE (Mean Absolute Percentage Error)\n",
        "4. **Generate prediction intervals** to quantify uncertainty\n",
        "\n",
        "The decomposition and autocorrelation analysis we've done here tells you *which* model to choose and *what* parameters to use."
      ],
      "metadata": {
        "id": "forecasting_next_steps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Summary: Key Concepts and Commands\n",
        "\n",
        "### Time Series Data Handling\n",
        "\n",
        "| Task | Command |\n",
        "|------|--------|\n",
        "| Convert to datetime | `pd.to_datetime(df['column'])` |\n",
        "| Set datetime index | `df.set_index('date_column')` |\n",
        "| Resample to different frequency | `df.resample('W').sum()` |\n",
        "| Percentage change | `df.pct_change()` |\n",
        "\n",
        "### Resampling Frequency Codes\n",
        "\n",
        "| Code | Meaning |\n",
        "|------|--------|\n",
        "| `'D'` | Daily |\n",
        "| `'W'` | Weekly |\n",
        "| `'MS'` / `'ME'` | Month start / end |\n",
        "| `'QE'` | Quarter end |\n",
        "| `'YE'` | Year end |\n",
        "\n",
        "### Analysis Tools\n",
        "\n",
        "| Task | Command |\n",
        "|------|--------|\n",
        "| Autocorrelation at specific lag | `series.autocorr(lag=n)` |\n",
        "| Lag plot | `pd.plotting.lag_plot(series, lag=n)` |\n",
        "| Autocorrelation plot | `pd.plotting.autocorrelation_plot(series)` |\n",
        "| Seasonal decomposition | `seasonal_decompose(series, model='multiplicative', period=n)` |\n",
        "\n",
        "### Window Functions\n",
        "\n",
        "| Task | Command |\n",
        "|------|--------|\n",
        "| Rolling window | `series.rolling(n).mean()` |\n",
        "| Expanding window | `series.expanding().mean()` |\n",
        "| Exponential weighting | `series.ewm(halflife=n).mean()` |"
      ],
      "metadata": {
        "id": "summary"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Appendix (Optional): Periodogram Analysis\n",
        "\n",
        "The **periodogram** is a tool from signal processing that identifies the most important periodicities in data. It's typically applied to detrended data. For most business data, daily, weekly, and yearly are the three periods to consider—but a periodogram can reveal unexpected patterns."
      ],
      "metadata": {
        "id": "zGFzQqFCZF26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import periodogram\n",
        "\n",
        "# Create a periodogram from the detrended sales time series\n",
        "timeseries = df[\"Sales\"] / decomposition.trend\n",
        "freqs, psd = periodogram(timeseries.dropna())\n",
        "\n",
        "# Create a dataframe with the results\n",
        "# Convert frequencies to periods (time between events)\n",
        "prd = pd.DataFrame({\"freqs\": freqs, \"psd\": psd})\n",
        "prd['period'] = 1/prd['freqs']\n",
        "\n",
        "# Plot the results\n",
        "prd.plot(\n",
        "    x='period',\n",
        "    y='psd',\n",
        "    ylabel='Power Spectral Density',\n",
        "    xlabel='Period (months)',\n",
        "    xlim=(0, 24),\n",
        "    # logx=True,\n",
        "    title='Periodogram of Wine Sales'\n",
        ")\n",
        "\n",
        "print(\"The peak at period ≈ 12 confirms the strong annual seasonality.\")"
      ],
      "metadata": {
        "id": "0jvGBedbZHft"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}